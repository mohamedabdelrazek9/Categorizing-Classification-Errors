# -*- coding: utf-8 -*-
"""MNIST_Kmeans.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hjK2cVzbo3WN_5vSwXAm9LgZeJ1jFa7u
"""

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np

"""Data Inilization"""

(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()
print(train_images.shape)
plt.figure()
plt.imshow(train_images[1], cmap='gray')
plt.colorbar()
plt.grid(False)
plt.axis('off')
plt.show()

# Normalize pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0

train_images = train_images[..., tf.newaxis]
test_images = test_images[..., tf.newaxis]

print(train_images.shape)

"""Importing Model"""

model = tf.keras.models.load_model('model_x.h5')
# Show the model architecture
model.summary()

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

print(test_acc)

probability_model = tf.keras.Sequential([model, 
                                         tf.keras.layers.Softmax()])

predictions = probability_model.predict(train_images)



from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(train_labels, np.argmax(predictions,axis=1))
# Normalise
cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
avg_arr = np.zeros(10)
for i in range(10):
  np.put(avg_arr, i, cmn[i][i])
avg_arr = np.average(avg_arr)
fig, ax = plt.subplots(figsize=(10,10))
target_names =  ['0','1', '2', '3', '4', '5', '6', '7', '8','9']
sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=target_names, yticklabels=target_names)
plt.text( 1,-0.09, r'Overall Accuracy:', fontsize=12)
plt.title(avg_arr)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show(block=False)

"""confusion matrix


"""

from sklearn.metrics import confusion_matrix
import itertools
import matplotlib.pyplot as plt

cm = confusion_matrix(y_true=train_labels, y_pred=np.argmax(predictions,axis=1))
#cm = cm .astype('float') / cm.sum(axis=1)[:, np.newaxis]


def plot_confusion_matrix(cm, classes,
                        normalize=False,
                        title='Confusion matrix',
                        cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
            horizontalalignment="center",
            color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')



cm_plot_labels = ['0','1', '2', '3', '4', '5', '6', '7', '8','9']

plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')

train_labels[0:32]

train_images.shape


"""creating a small dataset for the false classified examples of the first class, this just an example to see if our method is working"""
train_false_images=np.ones_like(train_images[0:32])
index=0
a=0

for pred in range (60000):
  if(np.argmax(predictions[index]) != train_labels[index] and train_labels[index] == 0 ):
    print('index:',index)
    print('a',a)
    train_false_images[a]=train_images[index]
    a= a+1
    


  index=index+1

train_false_images.shape


"""Importing VGG16 Model for Embeddings extraction"""

from keras.preprocessing import image
from keras.applications.vgg16 import VGG16
from keras.applications.vgg16 import preprocess_input
import numpy as np
import cv2

model = VGG16(weights='imagenet', include_top=False)

x = train_false_images[0]
res = cv2.resize(x, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)

res = np.expand_dims(res, axis=0)

res = preprocess_input(res)
res.shape
stacked_img = np.stack((res,)*3, axis=-1)
vgg16_feature = model.predict(stacked_img)

print (vgg16_feature.shape)

stacked_img.shape

new_dataset = np.ones((32, 25088))

for i in range(train_false_images.shape[0]):

  res = cv2.resize(train_false_images[i], dsize=(224, 224), interpolation=cv2.INTER_CUBIC)
  res = np.expand_dims(res, axis=0)

  res = preprocess_input(res)
  stacked_img = np.stack((res,)*3, axis=-1)
  
  a = model.predict(stacked_img)
  a = a.flatten()
  new_dataset[i] = a

"""Applying Kmeans on the false trained  data"""

from sklearn.cluster import KMeans
k_means = KMeans(init = 'k-means++', n_clusters = 5, n_init = 35)
k_means.fit(new_dataset)

G = len(np.unique(k_means.labels_)) #Number of labels
#2D matrix  for an array of indexes of the given label
cluster_index= [[] for i in range(G)]
for i, label in enumerate(k_means.labels_,0):
    for n in range(G):
        if label == n:
            cluster_index[n].append(i)
        else:
            continue

cluster_index

"""Visualizing the created clusters """

"""Cluster 0"""

f, axarr = plt.subplots(nrows=1, ncols=len(cluster_index[0]), figsize=(15, 9))
for i in range(len(cluster_index[0])):
  axarr[i].imshow(train_false_images[cluster_index[0][i]].reshape(train_images.shape[1], train_images.shape[2]))

images0 = []
for h in range(len(cluster_index[0])):
  images0.append(train_false_images[cluster_index[0][h]])

avg0 = sum(images0) / len(images0)
plt.imshow(avg0.reshape(train_images.shape[1], train_images.shape[2]))

"""Cluster 1"""

f, axarr = plt.subplots(nrows=1, ncols=len(cluster_index[1]), figsize=(15, 9))
for i in range(len(cluster_index[1])):
  axarr[i].imshow(train_false_images[cluster_index[1][i]].reshape(train_images.shape[1], train_images.shape[2]))

images0 = []
for h in range(len(cluster_index[1])):
  images0.append(train_false_images[cluster_index[1][h]])

avg0 = sum(images0) / len(images0)
plt.imshow(avg0.reshape(train_images.shape[1], train_images.shape[2]))

"""Cluster 2"""

f, axarr = plt.subplots(nrows=1, ncols=len(cluster_index[2]), figsize=(15, 9))
for i in range(len(cluster_index[2])):
  axarr[i].imshow(train_false_images[cluster_index[2][i]].reshape(train_images.shape[1], train_images.shape[2]))

images0 = []
for h in range(len(cluster_index[2])):
  images0.append(train_false_images[cluster_index[2][h]])

avg0 = sum(images0) / len(images0)
plt.imshow(avg0.reshape(train_images.shape[1], train_images.shape[2]))

"""Cluster 3"""

f, axarr = plt.subplots(nrows=1, ncols=len(cluster_index[3]), figsize=(15, 9))
for i in range(len(cluster_index[3])):
  axarr[i].imshow(train_false_images[cluster_index[3][i]].reshape(train_images.shape[1], train_images.shape[2]))

images0 = []
for h in range(len(cluster_index[3])):
  images0.append(train_false_images[cluster_index[3][h]])

avg0 = sum(images0) / len(images0)
plt.imshow(avg0.reshape(train_images.shape[1], train_images.shape[2]))

"""Cluster 4"""

f, axarr = plt.subplots(nrows=1, ncols=len(cluster_index[4]), figsize=(15, 9))
for i in range(len(cluster_index[4])):
  axarr[i].imshow(train_false_images[cluster_index[4][i]].reshape(train_images.shape[1], train_images.shape[2]))

images0 = []
for h in range(len(cluster_index[4])):
  images0.append(train_false_images[cluster_index[4][h]])

avg0 = sum(images0) / len(images0)
plt.imshow(avg0.reshape(train_images.shape[1], train_images.shape[2]))